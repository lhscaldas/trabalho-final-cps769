Eu tenho um dataset composto por um banco de dados em sqlite. Ele possui 4 tabelas, bitrate_test, bitrate_train, rtt_test e rtt_train. Ignore por enquanto as tabelas _test e foque apenas nas _train. Segue abaixo o header delas. 

client,server,timestamp,bitrate
ba,ce,1717718915,3000
ba,ce,1717718916,66910
ba,ce,1717718916,294878
ba,ce,1717718916,351151
ba,ce,1717718916,329464
ba,ce,1717718916,388265

client,server,timestamp,rtt
ba,ce,1717718941,12.52
ba,ce,1717719217,12.59
ba,ce,1717719722,12.49
ba,ce,1717719841,12.49
ba,ce,1717720185,12.48

O bitrate é medido em rajadas para cada par cliente-servidor (cerca de 15 medidas com o mesmo timestamp). Já a latência (rtt) é medida em intervalos não-uniformes.


Modifique o código abaixo para criar um programa utilizando lang chain que receba do usuário uma pergunta em linguagem natural, gere a query sql necessária para pesquise na tabela correta e responda o usuário em linguagem natural. Utilize o conceito de Chain of Thoughts.

from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from typing import Union, Optional
import os
from env import OPENAI_API_KEY # key armazenada em um arquivo que está no .gitignore para manter o sigilo
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

class Joke(BaseModel):
    """Joke to tell user."""
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")

class ConversationalResponse(BaseModel):
    """Respond in a conversational manner. Be kind and helpful."""

    response: str = Field(description="A conversational response to the user's query")

class Response(BaseModel):
    output: Union[Joke, ConversationalResponse]

llm = ChatOpenAI(model="gpt-4o-mini")

structured_llm = llm.with_structured_output(Joke)

system = """You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") and the final punchline (the response to "<setup> who?").

Here are some examples of jokes:

example_user: Tell me a joke about planes
example_assistant: {{"setup": "Why don't planes ever get tired?", "punchline": "Because they have rest wings!", "rating": 2}}

example_user: Tell me another joke about planes
example_assistant: {{"setup": "Cargo", "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!", "rating": 10}}

example_user: Now about caterpillars
example_assistant: {{"setup": "Caterpillar", "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!", "rating": 5}}"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{input}")])

few_shot_structured_llm = prompt | structured_llm

few_shot_structured_llm.invoke("what's something funny about woodpeckers")

A LLM deverá ser capaz, entre outras coisas, de responder 2 tipos de perguntas:
- Média da taxa de bitrate em cada rajada para cada para par cliente-servidor.

- Medida da latência que coincide com uma rajada de bitrate. 





